import streamlit as st
import pandas as pd
import requests
import plotly.express as px
import time
import io
from transformers import pipeline
from fastapi import FastAPI, UploadFile, File
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from bs4 import BeautifulSoup
from starlette.responses import JSONResponse
from fastapi.testclient import TestClient
from backend.model import analyzer
import plotly.express as px
from wordcloud import WordCloud
from collections import Counter

STOP_WORDS = {"–∏", "–≤", "–Ω–∞", "—Å", "–ø–æ", "–∑–∞", "–∫", "–ø–æ–¥", "–æ—Ç", "—á—Ç–æ", "–∫–∞–∫", "–¥–ª—è", "—Ç–æ", "–∞", "–ª–∏", "–±—É–¥–µ—Ç", "–º–µ–Ω—è", "–±—É–¥–µ—Ç", "–ø–æ–∫–∞", "–º–æ–∂–µ—Ç", "—É–∂–µ", "—Ä–∞–∑", "–º—ã"
              "–Ω–µ", "–Ω–æ", "–¥–æ", "–∏–∑", "—É", "–∂–µ", "—Ç–∞–∫", "–≤—ã", "–æ–Ω", "–æ–Ω–∞", "–æ–Ω–∏", "—ç—Ç–æ", "–≤—Å–µ", "–ø—Ä–∏", "—è", "–µ—Å—Ç—å", "–¥–Ω—ë–º", "–Ω–µ", "–ø–æ—á–µ–º—É", "—Ç–æ–ª—å–∫–æ", "–Ω–µ–ø—Ä–∏–º–µ—Ä", "–Ω–∞—Å"
              "–º—ã", "–º–Ω–µ", "–º–æ–π", "–º–æ—è", "–º–æ—ë", "–º–æ–∏", "—Ç–≤–æ–π", "—Ç–≤–æ—è", "—Ç–≤–æ—ë", "—Ç–≤–æ–∏", "-", "—Å–ø–∞—Å–∏–±–æ", "—Ç—ã", "–æ—á–µ–Ω—å", "–Ω–∏", "–∏—Ö", "–≤—Å–µ–º", "—Ç–∞–∫–∏–µ", "–∏—Ö", "–∏–ª–∏", "–µ—Å–ª–∏", "—ç—Ç–∏", "–∫—Ä–æ–º–µ", "–ø—Ä–æ"}

# –ó–∞–ø—É—Å–∫ FastAPI –≤–Ω—É—Ç—Ä–∏ Streamlit
app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origin_regex=".*",
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

LABEL_MAPPING = {
    "LABEL_0": "bad",
    "LABEL_1": "neutral",
    "LABEL_2": "good",
}


def check_emotion(text):
    posit = [':)', ':d', '–ø–æ–∑–¥—Ä–∞–≤', '—Ä–æ–∂–¥–µ–Ω–∏—è', '–ø–æ–¥–∞—Ä', '–¥–Ω—é—Ö–æ–π', '=)', '—Å—á–∞—Å—Ç—å', ' –Ω–æ—Ä–º', '—Å–ø–∞—Å–∏–±–æ', '–æ—Ç–ª–∏—á–Ω', '–º–æ–ª–æ–¥–µ—Ü', '–º–æ–ª–æ–¥—Ü—ã']
    bad = [':(', '=(', '–ø—Ä–æ–±–ª–µ–º–∞', '—Å–ø–æ—Ä', '—ç—Ö,', '—ç—Ö)', 'forbid', '–Ω–µ—á–µ–≥–æ']
    for word in posit:
        if word in text.lower():
            return 'good'
    for word in bad:
        if word in text.lower():
            return 'bad'
    return None

class TextRequest(BaseModel):
    text: str

@app.post("/analyze/")
def analyze_text(request: TextRequest):
    sentiment, confidence = analyzer.predict(request.text)
    sentiment = LABEL_MAPPING.get(sentiment, "unknown")
    emotion_fix = check_emotion(request.text)
    if emotion_fix:
        sentiment = emotion_fix
    return JSONResponse(content={"sentiment": sentiment, "confidence": confidence})

@app.post("/analyze-file/")
async def analyze_file(file: UploadFile = File(...)):
    contents = await file.read()
    df = pd.read_excel(io.BytesIO(contents))
    if "MessageText" not in df.columns:
        return JSONResponse(content={"error": "–§–∞–π–ª –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –∫–æ–ª–æ–Ω–∫—É 'MessageText'"})
    df["text"] = df["MessageText"].fillna("").apply(lambda x: BeautifulSoup(x, "html.parser").get_text(strip=True)[:512])
    df["sentiment"], df["confidence"] = zip(*df["text"].map(analyzer.predict))
    df["sentiment"] = df["sentiment"].map(LABEL_MAPPING)
    df["emotion_fix"] = df["text"].map(check_emotion)
    df["sentiment"] = df.apply(lambda row: row["emotion_fix"] if row["emotion_fix"] else row["sentiment"], axis=1)

    result = df[["UserSenderId", "MessageText", "sentiment", "confidence"]]
    return JSONResponse(content=result.to_dict(orient="records"))

client = TestClient(app)

st.title("üîç –ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞")
if "df" not in st.session_state:
    st.session_state.df = None

text_input = st.text_area("–í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞:")
if st.button("–ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç"):
    if text_input.strip():
        response = client.post("/analyze/", json={"text": text_input})
        if response.status_code == 200:
            data = response.json()
            st.write(f"**–¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å:** {data['sentiment']}")
            st.write(f"**–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏:** {round(data['confidence'] * 100, 2)}%")
        else:
            st.error("–û—à–∏–±–∫–∞ API!")
    else:
        st.warning("–í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç!")

uploaded_file = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ XLSX-—Ñ–∞–π–ª —Å –∫–æ–ª–æ–Ω–∫–æ–π 'MessageText'", type=["xlsx"])
if uploaded_file and st.button("–ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ñ–∞–π–ª"):
    files = {"file": ("file.xlsx", uploaded_file.getvalue(), "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")}
    response = client.post("/analyze-file/", files=files)
    if response.status_code == 200:
        st.session_state.df = pd.DataFrame(response.json())
    else:
        st.error("–û—à–∏–±–∫–∞ API!")

if st.session_state.df is not None:
    df = st.session_state.df
    df["MessageText"] = df["MessageText"].apply(lambda x: BeautifulSoup(x, "html.parser").get_text(strip=True))
    fig = px.pie(df, names="sentiment", title="–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏", hole=0.3)
    st.plotly_chart(fig)
    keyword = st.text_input("üîç –ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º:", "")
    all_classes = ["–í—Å–µ –∫–ª–∞—Å—Å—ã"] + list(df["sentiment"].unique())
    selected_class = st.selectbox("–§–∏–ª—å—Ç—Ä –ø–æ –∫–ª–∞—Å—Å—É:", all_classes)
    filtered_df = df
    if keyword:
        filtered_df = filtered_df[filtered_df["MessageText"].str.contains(keyword, case=False, na=False)]
    if selected_class != "–í—Å–µ –∫–ª–∞—Å—Å—ã":
        filtered_df = filtered_df[filtered_df["sentiment"] == selected_class]
    st.write(filtered_df)


    # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–¥—Å—á–µ—Ç–∞ —á–∞—Å—Ç–æ—Ç—ã —Å–ª–æ–≤ –ø–æ –∫–ª–∞—Å—Å–∞–º
    def get_top_words_by_class(df, top_n=15):
        word_counts = []
        
        for sentiment_class in df["sentiment"].unique():
            subset = df[df["sentiment"] == sentiment_class]
        
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç —ç—Ç–æ–≥–æ –∫–ª–∞—Å—Å–∞ –∏ —Ä–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å–ª–æ–≤–∞
            words = " ".join(subset["MessageText"]).lower().split()

            # –§–∏–ª—å—Ç—Ä—É–µ–º —Å–ª–æ–≤–∞, —É–±–∏—Ä–∞—è —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
            words = [word for word in words if word not in STOP_WORDS]

            word_freq = Counter(words).most_common(top_n)  # –¢–û–ü-10 —Å–ª–æ–≤
        
            for word, count in word_freq:
                word_counts.append({"word": word, "count": count, "sentiment": sentiment_class})
    
        return pd.DataFrame(word_counts)

    # –ü–æ–ª—É—á–∞–µ–º —Ç–∞–±–ª–∏—Ü—É —Å —á–∞—Å—Ç–æ—Ç–æ–π —Å–ª–æ–≤
    word_df = get_top_words_by_class(df)

    # –°—Ç—Ä–æ–∏–º Treemap
    fig_treemap = px.treemap(word_df, 
                          path=["sentiment", "word"], 
                          values="count", 
                          title="–ü–æ–ø—É–ª—è—Ä–Ω—ã–µ —Å–ª–æ–≤–∞ –ø–æ –∫–ª–∞—Å—Å–∞–º",
                          color="sentiment", 
                          color_discrete_map={"positive": "green", "negative": "red", "neutral": "blue"})

    # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º –≤ Streamlit
    st.plotly_chart(fig_treemap)
